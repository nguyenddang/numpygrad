{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danga\\miniconda3\\envs\\pytorchnn\\Lib\\site-packages\\numpy\\lib\\shape_base.py:597\u001b[0m, in \u001b[0;36mexpand_dims\u001b[1;34m(a, axis)\u001b[0m\n\u001b[0;32m    594\u001b[0m     axis \u001b[38;5;241m=\u001b[39m (axis,)\n\u001b[0;32m    596\u001b[0m out_ndim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(axis) \u001b[38;5;241m+\u001b[39m a\u001b[38;5;241m.\u001b[39mndim\n\u001b[1;32m--> 597\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize_axis_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_ndim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    599\u001b[0m shape_it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(a\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    600\u001b[0m shape \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m axis \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(shape_it) \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(out_ndim)]\n",
      "File \u001b[1;32mc:\\Users\\danga\\miniconda3\\envs\\pytorchnn\\Lib\\site-packages\\numpy\\core\\numeric.py:1380\u001b[0m, in \u001b[0;36mnormalize_axis_tuple\u001b[1;34m(axis, ndim, argname, allow_duplicate)\u001b[0m\n\u001b[0;32m   1378\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;66;03m# Going via an iterator directly is slower than via list comprehension.\u001b[39;00m\n\u001b[1;32m-> 1380\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([\u001b[43mnormalize_axis_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margname\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m axis])\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_duplicate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(axis)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(axis):\n\u001b[0;32m   1382\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m argname:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "a = np.random.rand(2, 3, 3)\n",
    "np.expand_dims(a, axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.randn(2,3,3)\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "class Tensor:\n",
    "    def __init__(self, data, _children=(), grad_fn=None):\n",
    "        assert type(data) == np.ndarray or type(data) == list, f\"Data must be of type numpy.ndarray or list, not {type(data)}\"\n",
    "        self.data = data\n",
    "        self.shape = data.shape\n",
    "        self._prev = set(_children)\n",
    "        self.grad = 0\n",
    "        self._backward = lambda: None\n",
    "        self.grad_fn = grad_fn\n",
    "    \n",
    "\n",
    "    def __repr__(self):\n",
    "        data_str = np.array2string(self.data, separator=', ', prefix='tensor(', suffix=')', precision=4) # makes formatting nicer\n",
    "        return 'tensor(' + data_str + ', grad_fn=' + str(self.grad_fn) + ')'\n",
    "    \n",
    "    \n",
    "    def __add__(self, other):\n",
    "        assert type(other) == Tensor or type(other) == int or type(other) == float, f\"Expected other to be of type Tensor or int, not {type(other)}\"\n",
    "        if type(other) == int or type(other) == float:\n",
    "            other = Tensor(np.array(other))\n",
    "        out =  Tensor(self.data + other.data, _children=(self, other), grad_fn='AddBackward')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += 1. * out.grad\n",
    "            other.grad += 1. * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        assert type(other) == Tensor or type(other) == int or type(other) == float, f\"Expected other to be of type Tensor, int, or float, not {type(other)}\"\n",
    "        if type(other) == int or type(other) == float:\n",
    "            other = Tensor(np.array(other))\n",
    "        out =  Tensor(self.data * other.data, _children=(self, other), grad_fn='MulBackward')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __matmul__(self, other):\n",
    "        assert type(other) == Tensor, f\"Expected other to be of type Tensor, not {type(other)}\"\n",
    "        out =  Tensor(self.data @ other.data, _children=(self, other), grad_fn='MatmulBackward')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad @ np.swapaxes(other.data, -1, -2)\n",
    "            other.grad += (np.swapaxes(self.data, -1, -2) @ out.grad).sum(axis=0)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        assert type(other) == int or type(other)==float, f\"Exponent must be an integer or float, not {type(other)}\"\n",
    "        out =  Tensor(self.data ** other, _children=(self,), grad_fn='PowBackward')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += other * self.data ** (other-1) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        out = Tensor(np.exp(self.data), _children=(self,), grad_fn='ExpBackward')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += np.exp(self.data) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        out = Tensor(1 / (1 + np.exp(-self.data)), _children=(self,), grad_fn='SigmoidBackward')\n",
    "        \n",
    "        def backward():\n",
    "            self.grad += out.data * (1 - out.data) * out.grad\n",
    "        out._backward = backward\n",
    "        return out\n",
    "    \n",
    "    def tanh(self):\n",
    "        out = Tensor(np.tanh(self.data), _children=(self,), grad_fn='TanhBackward')\n",
    "        \n",
    "        def backward():\n",
    "            self.grad += (1 - out.data ** 2) * out.grad\n",
    "        out._backward = backward\n",
    "        return out\n",
    "    \n",
    "    def relu(self):\n",
    "        out = Tensor(np.maximum(0, self.data), _children=(self,), grad_fn='ReluBackward')\n",
    "        \n",
    "        def backward():\n",
    "            self.grad += (self.data > 0) * out.grad\n",
    "        out._backward = backward\n",
    "        return out \n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + -1.0 * other\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return self * (other**-1)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * -1.0\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        return other + -1.0 * self\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "    \n",
    "    def __rtruediv__(self, other):\n",
    "        return other * self**-1\n",
    "    \n",
    "    def backward(self):\n",
    "        # sort children in topological order\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        # chain rule here\n",
    "        self.grad = np.ones_like(self.data).astype(np.float32) # set grad of this node to 1s\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta = torch.randn(10, 3, 4, 6, requires_grad=True)\n",
    "tb = torch.randn(10, 3, 6, 4,requires_grad=True)\n",
    "tc = ta @ tb\n",
    "tc.backward(torch.ones_like(tc))\n",
    "\n",
    "a = Tensor(ta.detach().numpy())\n",
    "b = Tensor(tb.detach().numpy())\n",
    "c = a @ b\n",
    "c.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(torch.from_numpy(a.grad.astype(np.float32)), ta.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta = torch.randn(3, 4, requires_grad=True)\n",
    "tb = torch.randn(10,4,5,requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 5])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ta@tb).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcasted_axes_A = tuple(i for i, (a, c) in enumerate(zip(ta.shape, tb.shape)) if a == 1 and c > 1)\n",
    "broadcasted_axes_B = tuple(i for i, (b, c) in enumerate(zip(tb.shape, tc.shape)) if b == 1 and c > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((), ())"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcasted_axes_A, broadcasted_axes_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
