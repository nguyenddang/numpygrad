{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.32722926, 0.16768765, 0.12142732],\n",
       "        [0.17919395, 0.01238304, 0.24202342],\n",
       "        [0.2013328 , 0.06927574, 0.03885227]],\n",
       "\n",
       "       [[0.26537811, 0.13652925, 0.08521428],\n",
       "        [0.44312213, 0.19058646, 0.20816376],\n",
       "        [0.18485526, 0.04554242, 0.13010937]]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.rand(2, 3, 3)\n",
    "b = np.random.rand(3)\n",
    "b*a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.randn(2,3,3)\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "class Tensor:\n",
    "    def __init__(self, data, _children=(), grad_fn=None):\n",
    "        assert type(data) == np.ndarray or type(data) == list, f\"Data must be of type numpy.ndarray or list, not {type(data)}\"\n",
    "        self.data = data\n",
    "        self.shape = data.shape\n",
    "        self._prev = set(_children)\n",
    "        self.grad = 0\n",
    "        self._backward = lambda: None\n",
    "        self.grad_fn = grad_fn\n",
    "    \n",
    "\n",
    "    def __repr__(self):\n",
    "        data_str = np.array2string(self.data, separator=', ', prefix='tensor(', suffix=')', precision=4) # makes formatting nicer\n",
    "        return 'tensor(' + data_str + ', grad_fn=' + str(self.grad_fn) + ')'\n",
    "    \n",
    "    \n",
    "    def __add__(self, other):\n",
    "        assert type(other) == Tensor or type(other) == int or type(other) == float, f\"Expected other to be of type Tensor or int, not {type(other)}\"\n",
    "        if type(other) == int or type(other) == float:\n",
    "            other = Tensor(np.array(other))\n",
    "        out =  Tensor(self.data + other.data, _children=(self, other), grad_fn='AddBackward')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += 1. * out.grad\n",
    "            other.grad += 1. * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        assert type(other) == Tensor or type(other) == int or type(other) == float, f\"Expected other to be of type Tensor, int, or float, not {type(other)}\"\n",
    "        if type(other) == int or type(other) == float:\n",
    "            other = Tensor(np.array(other))\n",
    "        out =  Tensor(self.data * other.data, _children=(self, other), grad_fn='MulBackward')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __matmul__(self, other):\n",
    "        assert type(other) == Tensor, f\"Expected other to be of type Tensor, not {type(other)}\"\n",
    "        out =  Tensor(self.data @ other.data, _children=(self, other), grad_fn='MatmulBackward')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad @ np.swapaxes(other.data, -1, -2)\n",
    "            other.grad += (np.swapaxes(self.data, -1, -2) @ out.grad).sum(axis=0)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        assert type(other) == int or type(other)==float, f\"Exponent must be an integer or float, not {type(other)}\"\n",
    "        out =  Tensor(self.data ** other, _children=(self,), grad_fn='PowBackward')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += other * self.data ** (other-1) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        out = Tensor(np.exp(self.data), _children=(self,), grad_fn='ExpBackward')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += np.exp(self.data) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        out = Tensor(1 / (1 + np.exp(-self.data)), _children=(self,), grad_fn='SigmoidBackward')\n",
    "        \n",
    "        def backward():\n",
    "            self.grad += out.data * (1 - out.data) * out.grad\n",
    "        out._backward = backward\n",
    "        return out\n",
    "    \n",
    "    def tanh(self):\n",
    "        out = Tensor(np.tanh(self.data), _children=(self,), grad_fn='TanhBackward')\n",
    "        \n",
    "        def backward():\n",
    "            self.grad += (1 - out.data ** 2) * out.grad\n",
    "        out._backward = backward\n",
    "        return out\n",
    "    \n",
    "    def relu(self):\n",
    "        out = Tensor(np.maximum(0, self.data), _children=(self,), grad_fn='ReluBackward')\n",
    "        \n",
    "        def backward():\n",
    "            self.grad += (self.data > 0) * out.grad\n",
    "        out._backward = backward\n",
    "        return out \n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + -1.0 * other\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return self * (other**-1)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * -1.0\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        return other + -1.0 * self\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "    \n",
    "    def __rtruediv__(self, other):\n",
    "        return other * self**-1\n",
    "    \n",
    "    def backward(self):\n",
    "        # sort children in topological order\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        # chain rule here\n",
    "        self.grad = np.ones_like(self.data).astype(np.float32) # set grad of this node to 1s\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta = torch.randn(10, 3, 4, 6, requires_grad=True)\n",
    "tb = torch.randn(10, 3, 6, 4,requires_grad=True)\n",
    "tc = ta @ tb\n",
    "tc.backward(torch.ones_like(tc))\n",
    "\n",
    "a = Tensor(ta.detach().numpy())\n",
    "b = Tensor(tb.detach().numpy())\n",
    "c = a @ b\n",
    "c.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(torch.from_numpy(a.grad.astype(np.float32)), ta.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta = torch.randn(3, 4, requires_grad=True)\n",
    "tb = torch.randn(10,4,5,requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 5])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ta@tb).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcasted_axes_A = tuple(i for i, (a, c) in enumerate(zip(ta.shape, tb.shape)) if a == 1 and c > 1)\n",
    "broadcasted_axes_B = tuple(i for i, (b, c) in enumerate(zip(tb.shape, tc.shape)) if b == 1 and c > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((), ())"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcasted_axes_A, broadcasted_axes_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
