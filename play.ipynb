{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.5505,  2.2295,  0.3588],\n",
       "          [ 1.6500,  0.5570,  0.0745],\n",
       "          [-0.3352,  0.6276, -2.1292]],\n",
       " \n",
       "         [[ 0.4529, -2.0799, -1.2591],\n",
       "          [ 0.6653, -0.4961,  0.5000],\n",
       "          [-0.7080, -1.0767, -1.5270]]]),\n",
       " tensor([[[4.7136, 9.2949, 1.4317],\n",
       "          [5.2071, 1.7455, 1.0774],\n",
       "          [0.7152, 1.8732, 0.1189]],\n",
       " \n",
       "         [[1.5729, 0.1249, 0.2839],\n",
       "          [1.9451, 0.6089, 1.6487],\n",
       "          [0.4926, 0.3407, 0.2172]]]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(2,3,3)\n",
    "a,torch.exp(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0055, 1.7234, 1.0141],\n",
       "        [8.2085, 0.6610, 0.4452]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "class Tensor:\n",
    "    def __init__(self, data, _children=(), grad_fn=None):\n",
    "        assert type(data) == np.ndarray or type(data) == list, f\"Data must be of type numpy.ndarray or list, not {type(data)}\"\n",
    "        self.data = data\n",
    "        self.shape = data.shape\n",
    "        self._prev = set(_children)\n",
    "        self.grad = 0\n",
    "        self._backward = lambda: None\n",
    "        self.grad_fn = grad_fn\n",
    "    \n",
    "\n",
    "    def __repr__(self):\n",
    "        data_str = np.array2string(self.data, separator=', ', prefix='tensor(', suffix=')', precision=4) # makes formatting nicer\n",
    "        return 'tensor(' + data_str + ', grad_fn=' + str(self.grad_fn) + ')'\n",
    "    \n",
    "    \n",
    "    def __add__(self, other):\n",
    "        assert type(other) == Tensor or type(other) == int or type(other) == float, f\"Expected other to be of type Tensor or int, not {type(other)}\"\n",
    "        if type(other) == int or type(other) == float:\n",
    "            other = Tensor(np.array(other))\n",
    "        out =  Tensor(self.data + other.data, _children=(self, other), grad_fn='AddBackward')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += 1. * out.grad\n",
    "            other.grad += 1. * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        assert type(other) == Tensor or type(other) == int or type(other) == float, f\"Expected other to be of type Tensor, int, or float, not {type(other)}\"\n",
    "        if type(other) == int or type(other) == float:\n",
    "            other = Tensor(np.array(other))\n",
    "        out =  Tensor(self.data * other.data, _children=(self, other), grad_fn='MulBackward')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __matmul__(self, other):\n",
    "        assert type(other) == Tensor, f\"Expected other to be of type Tensor, not {type(other)}\"\n",
    "        out =  Tensor(self.data @ other.data, _children=(self, other), grad_fn='MatmulBackward')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad @ np.swapaxes(other.data, -1, -2)\n",
    "            other.grad += (np.swapaxes(self.data, -1, -2) @ out.grad).sum(axis=0)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        assert type(other) == int or type(other)==float, f\"Exponent must be an integer or float, not {type(other)}\"\n",
    "        out =  Tensor(self.data ** other, _children=(self,), grad_fn='PowBackward')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += other * self.data ** (other-1) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        out = Tensor(np.exp(self.data), _children=(self,), grad_fn='ExpBackward')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += np.exp(self.data) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        out = Tensor(1 / (1 + np.exp(-self.data)), _children=(self,), grad_fn='SigmoidBackward')\n",
    "        \n",
    "        def backward():\n",
    "            self.grad += out.data * (1 - out.data) * out.grad\n",
    "        out._backward = backward\n",
    "        return out\n",
    "    \n",
    "    def tanh(self):\n",
    "        out = Tensor(np.tanh(self.data), _children=(self,), grad_fn='TanhBackward')\n",
    "        \n",
    "        def backward():\n",
    "            self.grad += (1 - out.data ** 2) * out.grad\n",
    "        out._backward = backward\n",
    "        return out\n",
    "    \n",
    "    def relu(self):\n",
    "        out = Tensor(np.maximum(0, self.data), _children=(self,), grad_fn='ReluBackward')\n",
    "        \n",
    "        def backward():\n",
    "            self.grad += (self.data > 0) * out.grad\n",
    "        out._backward = backward\n",
    "        return out \n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + -1.0 * other\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return self * (other**-1)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * -1.0\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        return other + -1.0 * self\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "    \n",
    "    def __rtruediv__(self, other):\n",
    "        return other * self**-1\n",
    "    \n",
    "    def backward(self):\n",
    "        # sort children in topological order\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        # chain rule here\n",
    "        self.grad = np.ones_like(self.data).astype(np.float32) # set grad of this node to 1s\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta = torch.randn(10, 3, 4, 6, requires_grad=True)\n",
    "tb = torch.randn(10, 3, 6, 4,requires_grad=True)\n",
    "tc = ta @ tb\n",
    "tc.backward(torch.ones_like(tc))\n",
    "\n",
    "a = Tensor(ta.detach().numpy())\n",
    "b = Tensor(tb.detach().numpy())\n",
    "c = a @ b\n",
    "c.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(torch.from_numpy(a.grad.astype(np.float32)), ta.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta = torch.randn(3, 4, requires_grad=True)\n",
    "tb = torch.randn(10,4,5,requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 5])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ta@tb).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcasted_axes_A = tuple(i for i, (a, c) in enumerate(zip(ta.shape, tb.shape)) if a == 1 and c > 1)\n",
    "broadcasted_axes_B = tuple(i for i, (b, c) in enumerate(zip(tb.shape, tc.shape)) if b == 1 and c > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((), ())"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcasted_axes_A, broadcasted_axes_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
